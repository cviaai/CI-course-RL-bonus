{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['              exposure_absolute 0x009a0902 (int)    : min=4 max=1250 step=1 default=156 value=156 flags=inactive']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install stable-baselines3[extra]\n",
    "a = !v4l2-ctl --device /dev/video0 --all | grep exposure_absolute\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\r\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps with the Gym interface\n",
    "\n",
    "An environment that follows the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) is quite simple to use.\n",
    "It provides to this user mainly three methods:\n",
    "- `reset()` called at the beginning of an episode, it returns an observation\n",
    "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether the episode is over and additional information\n",
    "- (Optional) `render(method='human')` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `method='rbg_array'` to retrieve an image of the scene\n",
    "\n",
    "Under the hood, it also contains two useful properties:\n",
    "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
    "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
    "\n",
    "The best way to learn about gym spaces is to look at the [source code](https://github.com/openai/gym/tree/master/gym/spaces), but you need to know at least the main ones:\n",
    "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
    "```python\n",
    "# Example for using image as input:\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "```                                       \n",
    "\n",
    "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
    "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
    "\n",
    "\n",
    "\n",
    "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
    "\n",
    "Below you can find an example of a custom environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-96c615b95efa>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-96c615b95efa>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    def __init__(self, arg1, arg2, ...):\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, arg1, arg2, ...):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        \n",
    "        #obtain min and max camera exposure values\n",
    "        # e.g. obtain your camera info with \n",
    "        # !v4l2-ctl --device /dev/video0 --all | grep exposure_absolute \n",
    "        # save the result in a string and extract min and max in 2 env variables\n",
    "        # round them to a multiple of 10 INSIDE their legal range, e.g. min = 4 -> min = 10\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # create an observation space with five values bounded from 0 to 1\n",
    "        # create the videocapture object, force the autoexposure to manual\n",
    "        # take some shots and throw them away \n",
    "        \n",
    "        # define an action space composed of only 2 actions, you want to increase or decrease the exposire time\n",
    "    \n",
    "    def step(self, action):\n",
    "        # here the environment process the received action\n",
    "        # increase or decrease by 10 the current  exposure time\n",
    "        # check if it's in the correct upper and lower bounds\n",
    "        # change the camera parameter to the new value, take one frame\n",
    "        # get the new resulting SSIM and return it for the observation\n",
    "        # give +1 if the SSIM is higher than the one of the preious step (yes you need to save it somewhere)\n",
    "        # give -1 if is lower than before\n",
    "        # give 5 points if is it is the maximum\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        # select random target exposure (target_exp) inside lower and upper exposure threshold that you found in _init_\n",
    "        # round that target exposure to a multiple of 10\n",
    "        # select a DIFFERENT starting exposure value (start_exp) and round it to multiple of 10\n",
    "        # set exposure to target_exp, record and throw away some frames, after take one frame\n",
    "        # this image (target_image) will be used to generate the metric for the RL algorithm\n",
    "        \n",
    "        # change exposure (again, throwing away some initial frames) to start_exp\n",
    "        # get the ssim between the taken frame and target_image, return it as the initial observation\n",
    "        \n",
    "        # the reward initially should be 0, the done set to False\n",
    "        \n",
    "        \n",
    "    return observation, reward, done\n",
    "    \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        # write inside here the code to render at video the collected frames\n",
    "        # add an empty black bar on tob where you print the SSIM betwwen the current image Vs the target one,\n",
    "        # the exposition current and target value\n",
    "    \n",
    "    \n",
    "    def close (self):\n",
    "        #close the video capture element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the env\n",
    "env = CustomEnv(arg1, ...)\n",
    "# Define and Train the agent\n",
    "model = A2C('CnnPolicy', env).learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "env = CustomEnv(arg1, ...)\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
